#!/usr/bin/env python3
"""
Run model script.
"""
import argparse
import importlib
import sys
sys.path.append('../')

import random
import torch
from dgl.data import register_data_args

import numpy as np
from pre_cora import load_cora
from core.app import App
from core.models.constants import AIFB, MUTAG, MUTAGENICITY, PTC_FM, PTC_FR, PTC_MM, PTC_MR
from core.models.constants import NODE_CLASSIFICATION, GRAPH_CLASSIFICATION
from core.data.constants import TRAIN_MASK, TEST_MASK, VAL_MASK
from utils.inits import to_cuda
from utils.io import read_params, create_default_path, remove_model

MODULE = 'core.data.{}'
AVAILABLE_DATASETS = {
    'dglrgcn',
    'dortmund'
}

def ten_cross(index,nodN,random_idx):

    test_start=index*(nodN//10)
    test_end=test_start+(nodN//10)
    train_idx = random_idx[:test_start]+random_idx[test_end:]
    test_idx = random_idx[test_start:test_end]


    def _idx_to_mask(idx, n):
        mask = np.zeros(n, dtype=int)
        mask[idx] = 1
        return torch.ByteTensor(mask)

    val_idx = train_idx[:len(train_idx) // 5]
    val_mask = _idx_to_mask(val_idx, nodN)

    train_idx = train_idx[len(train_idx) // 5:]
    train_mask = _idx_to_mask(train_idx, nodN)

    test_mask = _idx_to_mask(test_idx, nodN)

    return train_mask,test_mask,val_mask

def main(args):

    if args.gpu < 0:
        cuda = False
    else:
        cuda = True
        torch.cuda.set_device(args.gpu)

    default_path = create_default_path()
    print('\n*** Set default saving/loading path to:', default_path)

    if args.dataset == AIFB or args.dataset == MUTAG:
        module = importlib.import_module(MODULE.format('dglrgcn'))
        data = module.load_dglrgcn(args.data_path)
        data = to_cuda(data) if cuda else data
        mode = NODE_CLASSIFICATION
    elif args.dataset == MUTAGENICITY or args.dataset == PTC_MR or args.dataset == PTC_MM or args.dataset == PTC_FR \
            or args.dataset == PTC_FM or args.dataset == 'NCI1' or args.dataset == 'NCI109':
        module = importlib.import_module(MODULE.format('dortmund'))
        data = module.load_dortmund(args.data_path)
        data = to_cuda(data) if cuda else data
        mode = GRAPH_CLASSIFICATION
    elif args.dataset == 'cora' or args.dataset == 'citeseer':
        data = load_cora(args.data_path)
        data = to_cuda(data) if cuda else data
        mode = NODE_CLASSIFICATION
    else:
        raise ValueError('Unable to load dataset', args.dataset)
    # print_graph_stats(data[GRAPH])

    random_idx = [i for i in range(data['nodN'])]
    random.shuffle(random_idx)

    for i in range(10):

        data[TRAIN_MASK],data[TEST_MASK],data[VAL_MASK]=ten_cross(i,data['nodN'],random_idx)
        config_params = read_params(args.config_fpath, verbose=True)
        # 1. Training
        app = App()
        learning_config = {'lr': args.lr, 'n_epochs': args.n_epochs, 'weight_decay': args.weight_decay, 'batch_size': args.batch_size, 'cuda': cuda}
        print('\n*** Start training ***\n')
        app.train(data, config_params[0], learning_config, default_path, mode=mode)

        # 2. Testing
        print('\n*** Start testing ***\n')
        app.test(data, default_path, mode=mode)

        # 3. Delete model
        remove_model(default_path)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run graph neural networks.')
    register_data_args(parser)
    parser.add_argument("--config_fpath", type=str, required=True, 
                        help="Path to JSON configuration file.")
    parser.add_argument("--data_path", type=str, required=True,
                        help="Path from where to load the data (assuming they were preprocessed beforehand).")
    parser.add_argument("--gpu", type=int, default=1, help="gpu")
    parser.add_argument("--lr", type=float, default=1e-3,
                        help="learning rate")
    parser.add_argument("--n-epochs", type=int, default=200,
                        help="number of training epochs")
    parser.add_argument("--weight-decay", type=float, default=5e-4,
                        help="Weight for L2 loss")
    parser.add_argument("--batch-size", type=int, default=16, help="batch size (only for graph classification)")
    # parser.add_argument("--dataset", type=str, required=True, help="dataset name")
    args = parser.parse_args()

    main(args)
